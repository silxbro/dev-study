## [2-2] 컴퓨터가 이해하는 정보

이번 절에서는 0과 1만을 이해하는 컴퓨터가 어떻게 문자와 숫자를 인식하는지, 그리고 그렇게 표현된 정적인 데이터가 명령어에 의해 어떻게 실행되는지 학습해보자.

CPU는 기본적으로 0과 1만을 이해할 수 있다. 여기서 0과 1을 나타내는 가장 작은 정보의 단위를 **비트(bit)** 라고 한다.
1비트는 0 또는 1, 2개(2^1)의 정보를 표현할 수 있고, 2비트는 4개(2^2)의 정보, 3비트는 8개(2^3)의 정보를 표현할 수 있다.
즉, **N비트는 2^N개의 정보**를 표현할 수 있음을 알 수 있다.

<img src="https://github.com/user-attachments/assets/93c640a9-30af-4c91-9095-3916715fc70c" width="280"/><br/>

하지만 비트는 너무 작은 단위이다. 실행하는 프로그램은 수십만, 수백만 비트로 이루어져 있다.
그래서 프로그램의 크기를 말할 때는 비트보다 큰 단위인 바이트(byte), 킬로바이트(kB), 메가바이트(MB), 기가바이트(GB), 테라바이트(TB) 등을 사용한다.
테라바이트보다 더 큰 단위도 있지만, 범용적으로 사용되는 정보 단위는 최대 테라바이트까지인 경우가 많다.
<br/>

|구분|비트|
|:---|:---|
|1 byte|8 비트|
|1 kB|1,000 바이트|
|1 MB|1,000 킬로바이트|
|1 GB|1,000 메가바이트|
|1 TB|1,000 기가바이트|

**바이트(byte)** 는 여덟 비트를 묶은 단위를 말하므로 하나의 바이트로 표현할 수 있는 정보는 2^8 = 256개이다.
킬로바이트, 메가바이트, 기가바이트, 테라바이트 단위는 모두 이전 단위 1,000개를 묶은 단위를 말한다.

> 이전 단위 1,024개를 묶은 단위는 kiB, MiB, GiB, TiB로 표현한다.

비트, 바이트, 킬로바이트, 메가바이트, 기가바이트, 테라바이트는 모두 프로그램의 크기를 나타낼 때 사용하는 정보 단위이다. 프로그램의 관점에서 본 정보 단위다.
CPU 관점에서의 정보 단위로는 워드가 있다. **워드(word)** 란 CPU가 한 번에 처리할 수 있는 데이터의 크기를 의미한다.
프로그램의 크기가 2GB라고 해서 CPU도 한 번에 2GB를 읽어 들여 처리하는 것이 아니다. CPU는 프로그램을 워드 단위로 읽어 들이고 처리한다.
만약 CPU가 한 번에 16비트를 처리할 수 있다면 1워드는 16비트가 되고, 한 번에 32비트를 처리할 수 있다면 32비트가 되는 것이다.
워드의 크기는 CPU마다 다르지만, 현대 컴퓨터 대부분의 워드 크기는 32비트, 혹은 64비트이다.
<br/>
<br/>
### 🍓 데이터 - 0과 1로 숫자 표현하기
0과 1만을 이해할 수 있는 CPU는 컴퓨터 내부에서 **2진법(binary)** 을 사용해 2 이상, 0 이하의 수를 이해한다.
우리가 일상적으로 사용하는 **10진법(decimal)** 이 **숫자 9를 넘어가는 시점에 자리올림**해 0부터 9까지 10개의 숫자만으로 모든 수를 표현하듯, 컴퓨터가 사용하는 2진법 또는 **숫자 1을 넘어가는
시점에 자리올림**해 0과 1, 2개의 숫자만으로 모든 수를 표현한다. 2진수로 표현된 수는 숫자 뒤에 아래첨자로 (2)를 붙이거나 2진수 앞에 0b를 붙인다.

<img src="https://github.com/user-attachments/assets/340f4f43-89c1-42c0-ace2-f3c93c0d367e" width="250"/><br/>

> #### 16진법
> 2진법에는 단점이 있다. 표현하는 **숫자의 길이가 너무 길어진다**는 점이다. 가령 10진수 '128'을 2진수로 표현하면 '10000000(2)' 여덟 자리의 숫자가 필요하다.
> 그래서 컴퓨터가 이해하는 정보를 표현할 때는 2진수와 더불이 16진수도 함께 사용한다. 16진수를 나타내는 **16진법(hexadecimal)** 은 **숫자 15를 넘어가는 시점에 자리올림**을 하는 숫자 표현 방식이다.
> 16진법 체계에서는 10진수 10, 11, 12, 13, 14, 15를 각각 A, B, C, D, E, F로 표기한다.
>
> <img src="https://github.com/user-attachments/assets/c0b95f53-38fb-4c3c-962f-b3678a8e16a2" width="600"/><br/>
>
> 16진수로 표현된 수는 뒤에 아래첨자로 (16)을 부티거나 16진수 앞에 0x를 붙인다. 16진수도 2진수 못지않게 많이 활용되곤 한다.
> 소스 코드에 16진수를 직접 쓰기도 하고, '네트워크'에서 학습할 MAC 주소나 IPv6 주소를 표현할 때도 16진수를 사용한다.
>
> <img src="https://github.com/user-attachments/assets/f2a3e36b-f170-49b6-bba6-8370f281a95c" width="500"/><br/>
>
<br/>

이번에는 컴퓨터 내부에서 2진수로 **소수를 나타내는 방법**을 알아보자. 컴퓨터의 소수 표현을 학습할 때 가장 중요한 핵심은 표현하고자 하는 소수와 실제로 저장된 소수 간에 오차가 존재할 수 있다는 점이다.
이는 매우 단순한 소스 코드 실행을 통해 확인할 수 있는데, 다음과 같이 매우 단순한 파이썬 소스 코드를 예로 들어 보겠다.
변수 a에는 0.1, 변수 b에는 0.2, 변수 c에는 0.3을 저장하고, a와 b의 합이 c와 같을 때는 "Equal"을, 같지 않을 때는 "Not Equal"을 출력하는 코드이다.
구체적인 문법은 중요하지 않다. 코드의 대략적인 의미만 파악해 보길 바란다.
<br/>

#### [arch/floating.py]
```python
a = 0.1
b = 0.2
c = 0.3

if a + b == c:
    print("Equal")
else:
    print("Not Equal")
```
제시된 소스 코드의 결과를 당연히 'Equal'일 것이라고 생각할 수 있지만, 놀랍게도 결과는 'Not Equal'이다. 이러한 오차는 비단 파이썬에서만 발생하는 것이 아니다.
C/C++, 자바, 자바스크립트 등 많은 프로그래밍 언어에서 'Not Equal'이 결과로 출력된다. 이러한 오차의 존재, 그 발생 원인을 알지 못한다면 코딩 테스트나 정밀도 높은 개발 업무에 제대로 대처할 수 없다.

이러한 오차가 발생하는 이유는 무엇일까? 컴퓨터 내부에서는 소수점을 나타내기 위해 대표적으로 **부동 소수점(floating point)** 표현 방식을 사용하는데, 이 방식의 정밀도에 한계가 있기 때문이다.
부동 소수점은 소수점이 고정되어 있지 않은 소수 표현 방식으로, 필요에 따라 소수점의 위치가 이동할 수 있고 유동적(floating)이라는 의미에서 부동 소수점이라는 이름이 붙었다.

예를 들어 10진수 123.123이라는 수를 m ✕ 10^n의 꼴로 나타내면 1.23123 ✕ 10^2으로 표현할 수도 있고, 1231.23 ✕ 10^(-1)으로 표현할 수도 있다.
여기서 제곱으로 표현된 2와 -1을 **지수(exponent)**, 기울임채로 표기한 1.23123과 1231.23을 **가수(significand)** 라고 한다.
이로써 소수점의 위치를 고정하지 않고도 같은 소수를 다양하게 표현할 수 있다는 사실을 알 수 있다.

2진수 체계에서는 소수를 이와 유사하게 m ✕ 2^n의 꼴로 나타낸다. 가령 107.6640625라는 10진수 소수가 있다고 가정해 보자.
계산기 등을 통해 알 수 있듯, 이를 2진수로 나타내면 1101011.1010101이다. 이 2진수 소수는 1.1010111010101 ✕ 2^6으로 표현할 수도 있고, 110101110.10101 ✕ 2^(-2)으로 표현할 수도 있다.
이 경우에 지수는 각각 6, -2이고, 가수는 1.1010111010101, 110101110.10101이다.
2의 지수가 양수일 때는 2^(소수점을 왼쪽으로 이동한 횟수), 2의 지수가 음수일 때는 2^(소수점을 오른쪽으로 이동한 횟수)라고 생각해도 된다.

오늘날 대부분의 컴퓨터는 2진수의 지수와 가수를 다음과 같은 형식으로 저장한다. 이와 같은 부동소수점 저장 방식을 **IEEE 754**라고 한다.

<img src="https://github.com/user-attachments/assets/fd041ece-c9e3-440d-b3ad-b9a9ef46f53f" width="600"/><br/>

> 부호(sign) 비트가 0이면 양수를 의미하고, 1이면 음수를 의미한다.

그림과 같은 형태로 소수가 저장된다고 할 때, 가수의 정수부에는 1로 통일된 **정규화한 수(normalized number)** 가 저장된다. 즉, 가수는 1.○○○...의 형태를 띄고 있다.
앞서 예로 들었던 2진수 1101011.1010101의 경우 110101110.10101 ✕ 2^(-2)이 아닌 1.1010111010101 ✕ 2^6으로 저장되는 셈이다. 가수가 1.○○○...의 형태이기 때문이다.

그럼 2^(지수) ✕ 1.○○○... 형태의 소수를 저장할 때는 **지수**에 해당하는 값과 ○○○...에 해당하는 **소수 부분(fraction)** 만을 저장하면 된다.
어차피 2^(지수)의 2와 1.○○○...의 1은 통일되어 있는 값이기 때문이다. 따라서 컴퓨터가 가수를 저장할 때는 (가수인 1.○○○에서 1을 제외한)○○○에 해당하는 소수 부분만 저장하게 된다.
가령 1.1010111010101 ✕ 2^6의 가수를 저장할 때는 1010111010101이 저장되는 것이다.

<img src="https://github.com/user-attachments/assets/e2e84f86-0153-4de5-9f94-7b9b0d655876" width="340"/><br/>

컴퓨터가 지수를 저장할 때는 **바이어스(bias)** 값이 더해져서 저장되며, 이때 바이어스 값은 2^(k-1) - 1(k는 지수의 비트 수)이다.
지수를 표현하기 위해 8비트가 사용되었다면 바이어스 값은 2^7 - 1인 127이고, 11비트가 사용되었다면 바이어스 값은 2^10 - 1인 1,023이다.
즉, 1.1010111010101 ✕ 2^6이 32비트로 저장될 때는 127+6인 133(2진수 10000101)으로 저장되는 셈이다.

<img src="https://github.com/user-attachments/assets/a95e424e-4b35-489f-8d4a-db1e265d1299" width="330"/><br/>

결과적으로 1101011.1010101(10진수 107.6640625)이라는 수는 다음과 같이 저장된다.

<img src="https://github.com/user-attachments/assets/98d3cce8-285d-4d4b-a26e-114598ecac4e" width="330"/><br/>

실제로도 그런지 확인해보자. 다음은 10진수 107.6640625가 저장된 모습을 16진수로 출력하는 파이썬 코드이다. 역시 코드에 사용된 파이썬 문법은 중요하지 않다.
소수가 어떻게 저장되어 있는지 확인하는 데 집중해 살펴보자.

#### [arch/dec_to_bin.py]
```python
import struct

print(struct.pack('>f', 107.6640625).hex())
```
#### [실행결과]
```
42d75400
```

> struct.pack()은 인자로 주어진 실수 107.6640625를 부동 소수점으로 변환하는 코드이고, hex()는 해당 결과를 16진수로 표현하는 코드이다.

실행 결과인 42d75400을 2진수로 표현하면 01000010110101110101010000000000으로, 앞서 그림에서 확인했던 값과 같은 값이 출력되었다.

여기서 유의할 점은 **10진수 소수를 2진수로 표현할 때, 10진수 소수와 2진수 소수의 표현이 딱 맞아떨어지지 않을 수 있다**는 점이다.
예를 들어 1/3이라는 분수를 m ✕ 3^n의 꼴로 나타내고 싶다면 간단하게 1 ✕ 3^(-1)으로 표현하면 된다. 하지만 이 분수를 10진수로 표현하기 위해 m ✕ 10^n의 꼴로 나타내기는 어렵다.
1/3이라는 분수를 10진수로 표현하려면 0.3333...처럼 무한히 많은 소수점이 필요하기 때문이다. m ✕ 3^n으로 표현 가능한 수 중에서 m ✕ 10^n로 딱 맞아떨어지지 않는 수가 있을 수 있다는 뜻이다.

마찬가지로 10진수 0.1은 m ✕ 10^n의 꼴로 간단하게 나타낼수 있지만, 같은 수를 1.m✕2^n의 꼴로 표현하려면 무한하게 많은 소수점이 필요하다.
컴퓨터 저장공간은 한정적이기 때문에 무한히 많은 소수점을 저장할 수 없다. 그래서 딱 맞아떨어지지 않는 소수를 표현할 때는 일부러 소수점을 생략하여 저장한다. 그래서 앞에서와 같은 오차가 발생하는 것이다.
<br/>
<br/>
### 🍓 데이터 - 0과 1로 문자 표현하기
컴퓨터가 이해할 수 있는 문자들의 집합은 **문자 집합(character set)** 이라고 한다.
그리고 문자 집합에 속한 문자를 컴퓨터가 이해하는 0과 1로 이루어진 문자 코드로 변환하는 과정을 **문자 인코딩(character encoding)** 이라고 한다.
동일한 문자 집합이라 하더라도 다양한 문자 인코딩 방법이 있을 수 있다. 반대로, 0과 1로 표현된 문자를 사람이 이해하는 문자로 변환하는 과정은 **문자 디코딩(character decoding)** 이라고 한다.

다음과 같이 웹사이트를 이용하다가 볼 수 있는 글자 깨짐 현상은 웹사이트가 특정 인코딩 방법을 지원하지 않거나, 인코딩된 문자를 디코딩하는 방법을 알지 못하는 경우에 흔히 발생하는 문제이다.

<img src="https://github.com/user-attachments/assets/c805c9bf-f572-49da-ad17-07dd2adad2b5" width="620"/><br/>

컴퓨터가 0과 1로 문자를 표현하는 방법을 학습하는 것은 다양한 문자 집합과 인코딩 방식을 학습하는 것과 같다.
아스키는 초창기 컴퓨터에서 사용하던 문자 집합 중 하나로, 영어의 알파벳과 아라비아 숫자, 일부 특수 문자를 포함한다.

하나의 아스키 문자를 표현하기 위해서는 8비트(1바이트)를 사용한다.
8비트 중 1비트는 **패리티 비트(parity bit)** 라고 불리는데, 이는 오류 검출을 위해 사용되는 비트이기 때문에 실질적으로 문자 표현을 위해 사용되는 비트는 7비트이다.
7비트로 표현할 수 있는 정보의 가짓수는 2^7개이므로 총 128개의 문자를 표현할 수 있다.

다음의 **아스키 코드표**를 보면 알 수 있듯, 아스키 문자들은 0부터 127까지의 숫자 중 하나의 고유한 수에 대응된다.
아스키 문자에 대응된 고유한 수를 **아스키 코드**라고 한다. 우리는 아스키 코드를 2진수로 표현함으로써 아스키 문자를 0과 1로 대응시킬 수 있다. 이것이 아스키 코드의 인코딩 방식이다.
예를 들어 'A'는 10진수 65(2진수 1000001(2))로 인코딩되고, 'a'는 10진수 97(2진수 1100001(2))로 인코딩된다.

<img src="https://github.com/user-attachments/assets/bd08799a-a05f-4774-87d4-f8c51bc3f6f1" width="630"/><br/>
<img src="https://github.com/user-attachments/assets/73db8619-a1f3-4c8f-a22a-b8ab48ae3253" width="630"/><br/>

> 문자 인코딩에서 '글자에 부여된 고유한 값'을 **코드 포인트(code point)** 라고 한다. 가령 아스키 문자 'A'의 코드 포인트는 65이다.

다만 아스키 코드는 한글을 표기할 수 없다. 그래서 등장한 한글 인코딩 방식 중 하나가 EUC-KR이다.
EUC-KR은 KS X 10001, KS X 1003이라는 문자 집합 기반의 인코딩 방식으로, 아스키 문자를 표현할 때는 1바이트, 하나의 한글 글자를 표현할 때는 **2바이트** 크기의 코드를 부여한다.
2바이트(16비트)는 네 자리 16진수로 표현할 수 있으므로 EUC-KR로 인코딩된 한글 글자 하나는 네 자리 16진수로 나타낼 수 있다.

다음은 EUC-KR로 인코딩된 글자 '한글'의 일부이다. '한'의 경우 0xc7d0 행의 두 번째 열, 즉 0xc7d1로 인코딩되고, '글'의 경우 0xb1d0 행의 열두 번째 열, 즉 0xb1db로 인코딩된다.

<img src="https://github.com/user-attachments/assets/259d139a-87bf-456b-a19d-7a040ed13448" width="600"/><br/>

소스 코드로 직접 확인해 보자. 다음은 '한'과 '글'을 EUC-KR로 인코딩한 값을 각각 변수 a, b에 저장하고 출력하는 파이썬 코드이다.
실행 결과를 보면 16진수 c7d1과 b1db가 출력되는 것을 확인할 수 있다.

#### [arch/euckr.py]
```python
a = '한'.encode('euc-kr')
b = '글'.encode('euc-kr')
print(a.hex())
print(b.hex())
```

#### [실행 결과]
```
c7d1
b1db
```

EUC-KR 인코딩 방식을 사용하면 총 2,350개 정도의 한글 단어를 표현할 수 있다. 아스키 코드에 비해 표현할 수 있는 문자가 많아졌지만, **아직도 모든 한글 조합을 표현할 수 있을 정도로 많은 양은 아니다.**
문자 집합에 정의되지 않은 '쀓', '똠'과 같은 글자는 EUC-KR로 표현할 수 없다.

그래서 등장한 것이 **유니코드(unicode)** 문자 집합이다. 유니코드는 하늘을 포함해 EUC-KR에 비해 훨씬 많은 언어, 특수문자, 화살표, 이모티콘까지 코드로 표현할 수 있는 **통일된 문자 집합**이다.
유니코드가 없었다면 각각의 언어마다 다른 문자 집합과 인코딩 방식을 이해해야 했겠지만, 유니코드가 대부분의 언어를 지원하기 때문에 국가별로 다른 문자 집합과 인코딩 방식을 준비할 필요가 없어졌다.
이런 점에서 유니코드는 현대 가장 많이 사용되는 표준 문자 집합이며, 문자 인코딩에 있어 매우 중요한 역할을 맡고 있다.

아스키 코드나 EUC-KR처럼 유니코드 문자 집합에 속한 문자에는 고유한 값이 부여되어 있다. 예를 들어 유니코드 문자 집합 상에서 '한', '글'이라는 글자에 부여된 값은 각각 0xD55C, 0xAE00이다.

<img src="https://github.com/user-attachments/assets/44a82c4c-f72c-4d10-91ea-7cc11172158f" width="550"/><br/>

물론 간단한 소스 코드를 통해서도 유니코드 코드 포인트를 얻을 수 있다. 다음은 '한'과 '글'의 16진수 형태의 코드 포인트를 출력하는 파이썬 코드이다.

#### [arch/unicode_codepoint.py]
```python
a = hex(ord('한'))
b = hex(ord('글'))
print(a)
print(b)
```

#### [실행 결과]
```
0xd55c
0xae00
```

> 간혹 유니코드 문자에 부여된 값 앞에 U+D55C, U+AE00처럼 U+라는 문자열을 붙이기도 하는데, 이는 **16진수 유니코드**를 표현할 때 사용하는 표기이다.

이 코드도 마찬가자로 파이썬 문법은 중요하지 않다. 다른 프로그래밍 언어로도 얼마든지 구현할 수 있다. 다음은 자바 코드 예시이다. 중요한 것으느 프로그래밍 언어를 통해 코드 포인트를 얻어낼 수 있다는 것이다.

#### [arch/UnicodeCodepoint.java]
```java
public class UnicodeCodepoint {
    public static void main(String[] args) {

        int cp1 = "한".codePointAt(0);
        System.out.println("한: " + Integer.toHexString(cp1).toUpperCase());

        int cp2 = "글".codePointAt(0);
        System.out.println("글: " + Integer.toHexString(cp2).toUpperCase());
    }
}
```

아스키 코드나 EUC-KR은 글자에 부여된 값을 그대로 인코딩 값으로 삼았지만, 유니코드는 조금 다르다. 글자에 부여된 값 자체를 인코딩된 값으로 삼지 않고, 이 값을 다양한 방법으로 인코딩한다.
이러한 인코딩 방법에는 UTF-8, UTF-16, UTF-32 등이 있다. 요컨대, 유니코드가 문자의 집합이라면 UTF-8, UTF-16, UTF-32는 **유니코드 문자에 부여된 값을 인코딩하는 방식**을 말한다.

<img src="https://github.com/user-attachments/assets/dc996c8e-be44-4c6b-b5c8-93f4815dc9b8" width="500"/><br/>

UTF-8, UTF-16, UTF-32는 **가변 길이 인코딩** 방식이다. 인코딩된 결과의 길이가 일정하지 않을 수 있다는 의미이다.
같은 문자에 대해서도 어떻게 다른 인코딩 결과가 도출되는지 파이썬 코드를 통해 확인해보자.

#### [arch/unicode_encoding.py]
```python
a8 = '한'.encode('utf-8')
b8 = '글'.encode('utf-8')
print('utf-8 한:', a8.hex())
print('utf-8 글:', b8.hex())

a16 = '한'.encode('utf-16')
b16 = '글'.encode('utf-16')
print('utf-16 한:', a16.hex())
print('utf-16 글:', b16.hex())

a32 = '한'.encode('utf-32')
b32 = '글'.encode('utf-32')
print('utf-32 한:', a32.hex())
print('utf-32 글:', b32.hex())
```

#### [실행 결과]
```
utf-8 한: ed959c
utf-8 글: eab880
utf-16 한: fffe5cd5
utf-16 글: fffe00ae
utf-32 한: fffe00005cd50000
utf-32 글: fffe000000ae0000
```
마지막으로 base64 인코딩에 대해 알아보자. base64는 비단 문자뿐만 아니라, 이진 데이터까지 변환할 수 있는 인코딩 방식이다. 사실 문자보다는 이진 데이터를 인코딩하는 데에 더 많이 사용된다.
base64 인코딩은 이미지 등 단순 문자 이외의 데이터까지 모두 아스키 문자 형태로 표현할 수 있다.
사진 파일 등의 전송할 때 이메일 원문을 확인해 보면 다음과 같이 base64로 인코딩되어 있는 것을 확인할 수 있다.

<img src="https://github.com/user-attachments/assets/9fa2f7a7-5e5d-4d91-9ca4-c43603daf187" width="500"/><br/>

base64는 사실 64진법을 의미한다. 그래서 하나의 base64 인코딩 값을 표현하기 위해 64개의 문자가 사용되는 것이다.
2진수 하나를 표현하기 위해 2^1의 지수인 1비트가 필요하고 16진수 하나를 표현하기 위해 2^4의 지수인 4비트가 필요하듯, 64진수 하나를 표현하기 위해서는 2^6의 지수인 6비트가 필요하다.
따라서 변환할 데이터를 6비트씩 나누어 다음 표에 있는 하나의 문자로 변환하게 된다. 기본적으로 4개씩(24비트씩) 한 번에 변환된다.

<img src="https://github.com/user-attachments/assets/d86a508d-6bc2-4a37-bd0f-a1b3b16a09bf" width="600"/><br/>

예를 들어 'abc'라는 문자열이 있다고 가정해 보자. 앞서 학습했듯이 'a, b, c'는 각각 8비트의 아스키 코드 97, 98, 99로 인코딩될 수 있다.
각각의 아스키 코드는 8비트 크기의 2진수인 01100001, 01100010, 01100011로 표현할 수 있다. 즉, 문자열 'abc'는 총 24비트의 코드로 인코딩된다.
이를 base64 대응표에 따라 6비트씩 끊어서 변환하면 다음과 같이 'YWJj'가 된다.

<img src="https://github.com/user-attachments/assets/e12f461f-b089-44fe-9be2-d38c6f882698" width="600"/><br/>

문자열 'abc'는 6비트씩 나누어 총 24비트인 4개의 문자로('YWJj') 인코딩이 가능했다. 하지만, 반드시 6비트씩 나누어 떨어지지 않는 경우도 있다.
가령 문자열 'ab'의 경우 총 16비트이므로 4개씩 6비트로 나누어 떨어지지 않는다. 이 경우에는 나누어 떨어지지 않는 자리가 다음과 같이 0으로 채워지는 **패딩(padding)** 이 되고, 이는 '='로 인코딩된다.
다음 예시를 보자. 'ab'는 총 16비트로 표현되기 때문에 4개의 6비트로 변환되기에는 비트 수가 부족하다. 따라서 부족한 비트는 0으로 간주되어 '='으로 인코딩된다.
<br/>
<br/>
### 🍓 명령어
앞서 명령어는 **수행할 동작**과 **수행할 대상**으로 이루어져 있다고 했다.
여기서 수행할 대상은 다음과 같이 수행할 동작에 사용될 **데이터 자체**가 될 수도 있고, 동작에 사용될 **데이터가 저장된 위치**가 될 수도 있다.

이때 '명령어가 수행할 동작'은 **연산 코드(opcode)** 라고 하고, '동작에 사용될 데이터' 혹은 '(메모리나 레지스터의 주소와 같이)동작에 사용될 데이터가 저장된 위치'는 **오퍼랜드(operand)** 라고 한다.
즉, 하나의 명령어는 연산 코드와 0개 이상의 오퍼랜드로 구성되어 있으며, 명령어에서 연산 코드가 담기는 영역은 연산 코드 필드, 오퍼랜드가 담기는 영역은 오퍼랜드 필드라고 한다.

<img src="https://github.com/user-attachments/assets/8f5df11f-b40f-44d4-b139-c81765b7fd01" width="380"/><br/>

> 연산 코드는 연산자, 오퍼랜드는 피연산자라고도 부른다.

오퍼랜드와 관련해서는 기억해야 할 점이 있다.
오퍼랜드 필드에는 숫자나 문자와 같이 연산 코드에 사용될 데이터가 직접 명시되기보다는 많은 경우 **연산 코드에 사용될 데이터가 저장된 위치**, 즉 메모리 주소나 레지스터의 이름이 명시된다는 점이다.
그래서 오퍼랜드 필드를 **주소 필드(address field)** 라고 부르기도 한다. 그리고 만약 명령어에 사용된 오퍼랜드에 메모리 주소가 명시되었다면 이 명령어를 실행하기 위한 메모리 접근이 더 필요할 수 있다.

예를 들어 CPU가 메모리에 접근해 '더해라, 100번지 값에, 10을'이라는 명령어를 가지고 왔다(인출했다)고 하자.
CPU가 메모리로부터 인출한 명령어는 곧바로 실행될 수 없다. 이 명령어를 실행하려면 오퍼랜드 필드에 명시된 메모리 주소를 통해 한 번 더 메모리에 접근해야 한다.

<img src="https://github.com/user-attachments/assets/42abe761-c8bf-44b8-9ea4-d78bff555f76" width="470"/><br/>

연산 코드는 매우 다양하다. CPU에 따라 연산 코드의 구체적인 생김새가 다르다.
하지만 대부분의 CPU가 공통적으로 이해하는 대표적인 연산 코드의 유형에는 **데이터 전송, 산술/논리 연산, 제어 흐름 변경, 입출력 제어**가 있다.

<img src="https://github.com/user-attachments/assets/8123a31a-73d8-47ad-aca9-25143eb74f0c" width="550"/>
<img src="https://github.com/user-attachments/assets/8fbecee1-776a-4dc3-afa8-e1df83defa9d" width="550"/>

> #### 스택
> PUSH, POP 명령어는 스택에 대한 연산이다. 여기서 스택(stack)이란 한 쪽 끝이 막혀 있는 통과 같은 형태로 데이터를 관리하는 자료구조를 말한다.
> 컴퓨터 구조와 운영체제에서 자주 언급되는 개념이므로 미리 알아 두는 것이 좋다.
> 스택은 한 쪽 끝이 막혀 있기 때문에 데이터를 저장할 때는 막혀 있지 않은 쪽으로 차곡차곡 저장하고, 저장한 자료를 빼낼 때는 마지막으로 저장한 데이터부터 빼내서 관리한다.
> '나중에 저장한 데이터를 가장 먼저 빼내는 데이터 관리 방식(후입선출)'이라는 점에서 LIFO(Last In First Out) 자료구조라고도 부른다.
> 예를 들어 스택 안에 '1-2-3-4-5'의 순으로 데이터를 저장(PUSH)하면 데이터를 빼낼 때는 '5-4-3-2-1'의 순으로 빼낼(POP) 수 있다.

#### [기계어와 어셈블리어]
CPU는 0과 1로 표현된 데이터와 명령어를 이해할 수 있다. 이때 CPU가 이해할 수 있도록 0과 1로 표현된 정보를 있는 그대로 표현한 언어를 **기계어(machine code)** 라고 한다.
다음 검은 배경의 그림이 바로 0과 1로 표현된 온갖 데이터와 명령어의 모습이다.

<img src="https://github.com/user-attachments/assets/8ca289b7-d125-4e84-9818-26a5d39bd6f2" width="350"/>

하지만 기계어만 봐서는 이것이 어떤 프로그램인지, 어떻게 동작하는지 쉽사리 짐작하기가 어렵다. 그래서 등장한 언어가 **어셈블리어(assembly language)** 이다.
어셈블리어는 0과 1로 표현된 기계어를 읽기 편한 형태로 단순 번역한 언어이다. 어셈블리어를 보면 CPU가 이해할 수 있는 명령어의 종류와 동작을 파악할 수 있다.

<img src="https://github.com/user-attachments/assets/8fb71d6d-a1c4-4fae-bce5-687f3d62a8ff" width="230"/>

어떤 프로그래밍 언어로 어떤 프로그램을 만들든 컴퓨터 내부에서는 0과 1로 이뤄진 기계어로 변환하여 프로그램을 실행한다.
유의할 점은 구체적인 연산 코드의 종류나 레지스터의 이름, 명령어의 생김새가 CPU마다 다를 수 있다는 점이다. 코드를 통해 확인해 보자.
다음은 **어떤 수의 제곱을 반환하는 함수**를 나타내는 C 언어 소스 코드이다.
```C
int square(int num) {
    return num * num;
}
```
이 소스 코드 또한 컴퓨터 내부에서 기계어(명령어)로 변환되어 실행되며, 이 기계어를 표현한 어셈블리어는 다음과 같다. 소스 코드의 한 줄 한 줄이 CPU가 읽어 들이고, 해석하고, 실행하는 명령어인 셈이다.
앞쪽에 있는 초록색 글씨가 연산 코드, 남은 검은색 글씨가 오퍼랜드이다.

#### [CISC 기반 CPU의 어셈블리어]
<img src="https://github.com/user-attachments/assets/183cef47-96f7-4202-84d1-d7f6dfe81861" width="580"/>

명령어의 종류와 생김새는 CPU마다 다를 수 있다고 했다. 앞에서 제시했던 '어떤 수의 제곱을 반환하는 함수'를 나타내는 소스 코드는 다른 CPU에서 다음과 같이 다른 형태의 어셈블리어로 표현될 수 있다.
어셈블리어는 기계어를 그대로 번역한 언어이기 때문에 명령어의 종류와 생김새가 다르면 기계어도 달라지고, 이를 번역한 어셈블리어도 달라지게 된다.

#### [RISC 기반 CPU의 어셈블리어]
<img src="https://github.com/user-attachments/assets/55670192-5242-4ab8-a2c4-e03231549b19" width="580"/>

어셈블리어의 문법을 익히는 것은 관심이 있는 개발 직군에 따라 필요 여부가 달라질 수 있다. 하지만 같은 프로그램일지라도 CPU마다 이해하는 명령어가 다르면 실행이 불가할 수 있다는 점은 기억해 두어야 한다.
일례로, 인텔 CPU를 사용하는 컴퓨터에서 만들어진 실행 파일을 애플 CPU를 사용하는 컴퓨터에 그대로 옮겨서 실행할 수 없듯, 여러 플랫폼에서 실행되는 프로그램을 개발할 때는 특정 CPU에만 의존적인
코드로 만들지 않아야 한다.
<br/>

#### [명령어 사이클]
메모리 안에는 프로그램이 저장되어 있고, 이 프로그램은 여러 명령어로 구성되어 있다. 그리고 CPU는 이 메모리에서 명령어를 인출하고 실행하기를 반복하며 전체 프로그램을 실행해 나간다.
이때 CPU가 명령어를 처리하는 과정에는 정형화된 흐름이 있다.
즉, CPU가 명령어를 처리하는 과정에서 프로그램 속 각각의 명령어들은 일정한 주기를 반복하며 실행되는데, 바로 이 주기를 **명령어 사이클(instruction cycle)** 이라고 한다.

메모리에 저장된 명령어 하나를 실행하고 싶을 때 가장 먼저 해야 할 일은 무엇일까? 그 명령어를 메모리에서 CPU로 가지고 와야(인출해야) 한다.
이는 명령어 사이클의 첫 번째 과정으로, 메모리에 있는 명령어를 CPU로 가지고 오는 단계를 **인출 사이클(fetch cycle)** 이라고 한다.
CPU로 명령어를 인출했다면 이제는 그 명령어를 실행해야 한다. 명령어 사이클의 두 번째 과정인 CPU로 가져온 명령어를 실행하는 단계를 **실행 사이클(execution cycle)** 이라고 한다.
CPU는 메모리 속 명령어를 가져와 실행하고, 가져와 실행하기를 반복하며 프로그램을 실행하기 때문에 인출 사이클과 실행 사이클은 반복되게 된다.

<img src="https://github.com/user-attachments/assets/ece1cbda-9634-4a6f-a880-e12fdef906a2" width="170"/>
<br/>
<br/>
하지만 모든 명령어가 이렇게 간단하게 실행되지는 않는다. 명령어를 인출했더라도 곧바로 실행할 수 없는 경우가 있다. 다음과 같이 오퍼랜드 필드에 메모리 주소가 명시된 경우를 생각해 보자.
이 경우에는 CPU가 명령어를 인출했더라도 한 번 더 메모리에 접근해야 한다.

이렇게 명령어를 실행하기 위해 한 번 더 메모리에 접근하는 단계를 **간접 사이클(indirect cycle)** 이라고 한다.
어떤 명령어는 인출과 실행 사이클만으로 실행되고, 어떤 명령어는 인출, 간접, 실행 사이클을 모두 거쳐 실행된다.

<img src="https://github.com/user-attachments/assets/304d9632-f398-4473-b314-a8a47ed11031" width="390"/>

마지막으로 너무나 중요한 사이클이 하나 더 남아있다. 바로 인터럽트를 처리하는 **인터럽트 사이클**이다. 인터럽트 사이클을 이해하려면 CPU 레지스터에 대한 이해가 선행되어야 한다.