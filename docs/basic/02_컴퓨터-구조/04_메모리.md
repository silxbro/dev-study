## [2-4] 메모리

실행 중인 프로그램은 메모리에 저장되며, CPU는 메모리에 저장된 정보를 읽고, 쓰고, 실행한다. 이번에는 메모리의 하드웨어적 특징과 종류, 더불어 메모리의 2가지 주소 체계와 바이트 저장 순서에 대해 알아보자.
<br/>
<br/>
### 🍓 RAM
앞서 설명했던 내용을 되짚어보면, 메인 메모리 역할을 하는 하드웨어에는 RAM과 ROM이 있지만, (메인)메모리는 보통 RAM을 지칭하는 경우가 많다고 했다.
RAM은 전원을 끄면 저장하고 있던 데이터와 명령어가 날라가는 **휘발성 저장장치(volatile memory)** 이며, CPU가 실행할 대상을 저장하는 부품이라고도 했다.
이는 전원이 꺼져도 저장된 내용이 유지되는 **비휘발성 저장장치(non-volatile memory)** 이자, 보관할 대상을 저장하는 보조기억장치와는 대조적인 특징이다.

CPU는 보조기억장치에 저장된 프로그램을 곧장 가져와 실행할 수 없기 때문에 어떠한 프로그램을 실행하고자 한다면 프로그램을 보관하고 있는 보조기억장치에서 메모리로 복사해 와야 한다.
짐작할 수 있듯, 그래서 RAM의 용량은 컴퓨터에 큰 영향을 끼친다.
RAM의 용량이 작으면 보조기억장치로부터 실행할 프로그램을 가지고 오는 일이 잦아져 실행 시간이 길어지지만, RAM의 용량이 충분히 크면 보조기억장치로부터 많은 데이터를 가져와 미리 RAM에 저장할 수 있기
때문에 많은 프로그램을 동시에 실행하는 데 유리하다.

> 하지만 RAM의 용량이 필요 이상으로 커진다고 해서 반드시 컴퓨터의 성능이 그에 비례하여 향상되는 것은 아니다.

우선 메모리로 사용되는 하드웨어인 RAM의 이름이 왜 RAM인지부터 이해해야 한다.
RAM은 임의 접근 메모리(Random Access Memory)의 약자로, 여기서 **임의 접근(random access)** 이란 저장된 요소에 순차적으로 접근할 필요 없이 임의의 위치에 곧장 접근 가능한 방식을 의미한다.
그래서 **직접 접근(direct access)** 이라고도 부른다.
가령 100번지에 있는 데이터에 접근하고자 할 때 1번지, 2번지, 3번지, ..., 100번지의 순으로 접근할 필요 없이 곧장 100번지로 접근하는 것이 가능하다는 것이다.
그렇기 때문에 1번지에 접근하든 1000번지에 접근하든 데이터에 접근하는 시간이 동일하다는 특징이 있다.

임의 접근과 반대되는 개념으로는 순차 접근이 있다. **순차 접근(sequential access)** 은 이름 그대로 특정 위치에 저장된 요소에 접근하기 위해 처음부터 순차적으로 접근하는 방식이다.
당연히 이 경우에는 어떤 위치에 접근하느냐에 따라 데이터에 접근하는 시간이 달라질 수 있다.

<img src="https://github.com/user-attachments/assets/0bfafffe-9229-4e90-89b7-f48cc2bbdc0b" width="360"/><br/>

RAM의 종류에는 크게 DRAM, SRAM, SDRAM, DDR SDRAM 등이 있다. 하나씩 가볍게 알아보고 넘어가자.

#### [1] DRAM
DRAM(Dynamic RAM)의 Dynamic(동적인)은 말 그대로 저장된 데이터가 동적으로 변하는(사라지는) 특성을 의미한다. 즉, DRAM은 **시간이 지나면 저장된 데이터가 점차 사라지는 RAM**이다.
그렇기 때문에 DRAM은 데이터의 소멸을 막기 위해 일정 주기로 데이터를 재활성화(다시 저장)해야 한다. 이러한 단점에도 불구하고 DRAM을 메모리로 사용하는 것이 일반적이다.
비교적 DRAM의 소비 전력이 낮고, 저렴하며, 집적도가 높아 메모리를 대용량으로 설계하기에 용이하기 때문이다.

#### [2] SRAM
SRAM(Static RAM)의 Static(정적인)은 DRAM과는 달리, 저장된 데이터가 변하지 않는 RAM을 의미한다. 즉, **시간이 지나도 저장된 데이터가 사라지지 않는 RAM**이다.
참고로, 저장된 데이터가 사라지지 않는다고 해서 SRAM이 비휘발성 저장장치라는 것은 아니다. SRAM도 전원이 공급되지 않으면 저장된 내용이 소실되는 것은 마찬가지이다.

일반적으로 SRAM은 DRAM과 비교해 속도는 빠르지만, 소비 전력이 크고 가격도 비싼데다 집적도도 낮기 때문에 대용량으로 만들 필요는 없지만 속도가 빨라야 하는 저장장치, 가령 **캐시 메모리** 등에서 사용된다.

#### [3] SDRAM
SDRAM(Synchronous Dynamic RAM)은 **클럭 신호와 동기화**된, 보다 발전된 형태의 DRAM을 말한다(SRAM과 DRAM의 합성어가 아니라는 점에 주의하기 바란다).
클럭 신호와 동기화되었다는 것은 클럭 타이밍에 맞춰 CPU와 정보를 주고받을 수 있다는 것을 의미한다. 다시 말해, SDRAM은 클럭에 맞춰 작동하며 CPU와 정보를 주고받을 수 있는 DRAM을 말한다.

#### [4] DDR SDRAM
DDR SDRAM(Double Data Rate SDRAM)은 **대역폭을 넓혀 속도를 빠르게 만든 SDRAM**을 말한다. **대역폭(data rate)** 이란 '데이터를 주고받을 길의 너비'를 말한다.
SDRAM이 한 클럭당 한 번씩 CPU와 데이터를 주고받을 수 있다면, DDR SDRAM은 두 배의 대역폭으로 한 클럭당 두 번씩 CPU와 데이터를 주고받을 수 있다.
말하자면 DDR SDRAM은 **너비가 두 배인 자동차 도로**와 같다. 따라서 DDR SDRAM은 한 클럭당 하나씩 데이터를 주고받을 수 있는 SDRAM보다 전송 속도가 두 배 가량 빠르다.

> 한 클럭당 하나씩 데이터를 주고받을 수 있는 SDRAM은 SDR SDRAM(Single Data Rate SDRAM)이라고 부르기도 한다.

**DDR2 SDRAM**은 DDR SDRAM보다 대역폭이 두 배 넓은 SDRAM을 말한다. 따라서 DDR2 SDRAM은 SDR SDRAM보다 너비가 네 배인 자동차 도로와 같다.
같은 맥락으로 **DDR3 SDDRAM**은 대역폭이 DDR2 SDRAM보다 두 배 넓고, SDR SDRAM보다 여덟 배 넓은 SDRAM이다.
최근에 우리가 흔히 볼 수 있는 메모리는 **DDR4 SDRAM**으로, SDR SDRAM보다 열여섯 배 넓은 대역폭을 가진 SDRAM을 말한다.

<img src="https://github.com/user-attachments/assets/dc2c17c4-4e8f-4da1-a31b-ec5a236fc831" width="550"/><br/>
<br/>
### 🍓 메모리에 바이트를 밀어 넣는 순서 - 빅 엔디안과 리틀 엔디안
현대의 메모리는 대부분 데이터를 **바이트** 단위로 저장하고 관리한다.
하지만 메모리는 데이터를 CPU로부터 바이트 단위로 받아들이지 않고, 일반적으로 4바이트(32비트), 혹은 8바이트(64비트)인 워드 단위로 받아들인다.
그렇게 여러 바이트로 구성된 데이터를 받아들여 여러 주소에 걸쳐 저장하게 된다.
다시 말해, 한 주소에 1바이트씩을 저장하는 메모리는 4바이트의 데이터를 4개의 주소에 저장하고, 8바이트의 데이터를 8개 주소에 저장한다.

예를 들어 보자. 16진수인 1A2B3C4D는 1A, 2B, 3C, 4D로 나누어 4개의 주소에 저장되고, 16진수 1A2B3C4D5A6B7C8D는 1A, 2B, 3C, 4D, 5A, 6B, 7C, 8D로 나누어 8개의 주소에 저장되어야 한다.

> 16진수 하나를 저장하는 데에는 4비트가 필요하다(2^4=16). 따라서 16진수 2개를 저장하는 데에는 8비트인 1바이트가 필요하다.

이때 어떤 문화권에서는 글을 읽을 때 왼쪽에서 오른쪽의 순서로 읽고, 어떤 문화권에서는 오른쪽에서 왼쪽의 순서로 읽듯, 메모리에 바이트를 저장하는 방식은 연속해서 저장해야 하는 바이트를 어떤 순서로
저장하는지에 따라 빅 엔디안과 리틀 엔디안으로 나눌 수 있다.

**빅 엔디안(big endian)** 은 **낮은 번지의 주소에 상위 바이트부터 저장하는 방식**을 말한다. 여기서 상위 바이트는 가장 큰 값이라고 생각해도 무방하다.
10진수 123에서 가장 큰 수는 당연히 100을 나타내는 1일 것이다. 마찬가지로 16진수 1A2B3C4D에서 가장 큰 수, 최상위 바이트(최상위 8비트)는 1A이다.
예를 들어 다음 레지스터 안에 있는 1A2B3C4D라는 값을 메모리 a+2번지부터 빅 엔디안 방식으로 저장하면 낮은 주소 번지부터 1A, 2B, 3C, 4D의 순으로 저장된다.

<img src="https://github.com/user-attachments/assets/bfa566f7-0ce1-491a-a54f-037eb37e6a65" width="500"/><br/>
<br/>

반면, **리틀 엔디안(little endian)** 은 **낮은 번지의 주소에 하위 바이트부터 저장하는 방식**을 말한다. 하위 바이트는 상위 바이트와 반대로, 가장 작은 값을 의미한다.
10진수 123에서 가장 작은 값이 3이듯, 16진수인 1A2B3C4D의 최하위 바이트는 4D이다.
16진수 1A2B3C4D라는 값을 메모리 a+2번지부터 리틀 엔디안 방식으로 저장하면 다음과 같이 낮은 주소의 번지부터 4D, 3C, 2B, 1A의 순으로 저장된다.

<img src="https://github.com/user-attachments/assets/0c07bf8b-d8ee-49fe-9595-f0ed6e24afb0" width="500"/><br/>
<br/>

> #### MSB와 LSB
> **MSB(Most Significant Bit)** 는 숫자의 크기에 가장 큰 영향을 미치는 유효 숫자, 쉽게 말해 가장 왼쪽에 있는 비트를 말하고, 반대로 **LSB(Least Significant Bit)** 는 숫자의 크기에
> 가장 적은 영향을 미치는 유효 숫자, 쉽게 말해 가장 오른쪽에 있는 비트를 말한다. 예로 들었던 10진수 123에서 MSB쪽 숫자는 1이고, LSB쪽 숫자는 3이 될 것이다.
>
> 따라서 빅 엔디안에 MSB가 있는 바이트, 즉 중요하고 큰 데이터부터 저장해 나가는 방식이라면 리틀 엔디안은 LSB가 있는 바이트, 즉 덜 중요하고 작은 데이터부터 저장해 나가는 방식이라고 볼 수 있다.

리틀 엔디안과 빅 엔디안 사이에는 각각 분명하게 구분되는 장단점이 있다. 빅 엔디안은 우리가 일상적으로 숫자 체계를 읽고 쓰는 순서와 동일하기 때문에 메모리 값을 직접 읽거나, 특히 디버깅할 때 편리하다.
주소에 1A, 2B, 3C, 4D의 순서대로 저장된 값 그대로 16진수 1A2B3C4D로 읽으면 그만이기 때문이다.

반면, 리틀 엔디안은 메모리 값을 직접 읽고 쓰기는 불편하지만 수치 계산이 편리하다는 장점이 있다.
예를 들어 우리가 123 + 456이라는 덧셈을 수행할 때, 가장 작은 값인 일의 자릿수 3과 6부터 계산해 나가듯 가장 작은 값부터 저장되어 있는 데이터의 시작점에서 수치를 계산해 나가거나 자리올림할 수 있다.

> 컴퓨터 환경에 따라 빅 엔디안과 리틀 엔디안 중 하나로 결정되어 있는 경우도 있고, 빅 엔디안과 리틀 엔디안 중 하나를 선택할 수 있도록 설계되어 있는 경우도 있다. 이는 바이 엔디안(bi-endian)이라고 한다.

컴퓨터 환경이 빅 엔디안 또는 리틀 엔디안 중 어떤 방식을 활용하는 환경인지는 간단한 파이썬 코드를 통해 알 수 있다. 다음 코드를 실행해 'big'이 출력되면 빅 엔디안, 'little'이 출력되면 리틀 엔디안 환경이다.

#### [arch/endianness.py]
```python
import sys
print(sys.byteorder)
```

이전에(p50) 10진수 소수인 107.6640625는 16진수 42d75400으로 표현되며, 다음과 같은 파이썬 코드로 표현할 수 있다고도 설명했다.
사실 이 코드는 **빅 엔디안(>) 방식으로 표기된 107.6640625라는 소수(f)를 16진수(hex)로 표현하라**는 코드였다. 따라서 실행 결과도 '42d7540'이었다.

#### [arch/dec_to_bin.py]
```python
import struct

print(struct.pack('>f', 107.6640625).hex())
```

#### [실행 결과]
```
42d75400
```
반면, **리틀 엔디안(<) 방식으로 표기된 107.6640625라는 소수(f)를 16진수(hex)로 표현하라**는 파이썬 코드는 다음과 같다.
빅 엔디안으로 표현하면 '42d75400'인 10진수 107.6640625를 리틀 엔디안으로 표현한 실행 결과는 다음과 같다.

#### [arch/dec_to_bin2.py]
```python
import struct

print(struct.pack('<f', 107.6640625).hex())
```

#### [실행 결과]
```
0054d742
```

즉, 리틀 엔디안 방식으로 저장된 10진수 소수 107.6640625는 메모리 내에 '0054d742'로 저장되는 것을 볼 수 있다.
다음은 107.6640625를 리틀 엔디안 방식으로 저장된 변수를 디버깅한 모습이다. 실제 메모리에서 107.6640625를 저장한 데이터 '0054d742'를 확인할 수 있다.

<img src="https://github.com/user-attachments/assets/96cfb6f5-b1c6-447d-b7ba-464e986985e7" width="350"/><br/>
<br/>
<br/>
### 🍓 캐시 메모리
CPU는 프로그램 실행 과정에서 빈번히 메모리에 접근해야만 한다.
하지만 CPU가 메모리에 접근하는 속도는 CPU가 레지스터에 접근하는 속도보다 느리기 때문에 CPU의 연산 속도가 아무리 빨라도 메모리에 접근하는 속도가 느리면 CPU의 빠른 연산 속도는 아무 효옹이 없다.
그래서 등장한 저장장치가 바로 캐시 메모리이다.

**캐시 메모리(cache memory)** 는 CPU의 연산 속도와 메모리 접근 속도의 차이를 줄이기 위해 탄생한 저장장치로, CPU와 메모리 사이에 위치한 **SRAM 기반의 저장장치**이다.
CPU가 매번 메모리에 왔다 갔다 하는 시간이 오래 걸리므로 CPU가 사용할 일부 데이터를 미리 캐시 메모리로 가져와 활용하자는 것이다.

<img src="https://github.com/user-attachments/assets/bb0a9e00-d24a-4adc-b8e0-41cabb7bed51" width="620"/><br/>
<br/>

컴퓨터 내부에는 여러 종류의 캐시 메모리가 있다.
이 중 코어와 가장 가까운 캐시 메모리를 **L1 캐시(Level1 cache)**, 그 다음으로 가까운 캐시 메모리를 **L2 캐시(Level2 cache)**, 그 다음으로 가까운 캐시 메모리를
**L3 캐시(Level3 cache)** 라고 부른다. 일반적으로 L1 캐시와 L2 캐시는 코어 내부에, L3 캐시는 코어 외부에 위치해 있다.

<img src="https://github.com/user-attachments/assets/dc8fa6db-644d-4002-83f8-d23c75fabd0b" width="500"/><br/>
<br/>

캐시 메모리의 크기는 L1 < L2 < L3 의 순으로 크고, 속도는 L3 < L2 < L1의 순으로 빠르다.
CPU가 메모리 내에 데이터가 필요하다고 판단하면 우선 L1 캐시 메모리에 해당 데이터가 있는지 알아보고, 없다면 L2, L3 캐시 메모리 순으로 데이터를 검색한다.
윈도우를 사용하는 경우 [작업 관리자] 창의 [성능] 탭에서 L1, L2, L3 캐시 메모리의 크기를 확인할 수 있다. 캐시 메모리의 크기 또한 CPU 성능에 영향을 미치므로 CPU 규격을 판단할 때 함께 확인하면 좋다.

<img src="https://github.com/user-attachments/assets/cf634d95-c04e-4cf3-8b68-0de57a39fd3a" width="550"/><br/>
<br/>

멀티코어 프로세서의 경우 일반적으로 L1 캐시 메모리와 L2 캐시 메모리는 코어마다 고유한 캐시 메모리로 할당되고, L3 캐시는 여러 코어가 공유하는 형태로 구현된다.

<img src="https://github.com/user-attachments/assets/9a08a6cf-3762-4cde-90f2-ea9729c078a0" width="520"/><br/>
<br/>

또한 코어와 가장 가까운 L1 캐시 메모리는 명령어만을 저장하는 L1 캐시인 L1I 캐시와 데이터만을 저장하는 L1 캐시인 L1D 캐시로 구분하기도 하며, 이러한 유형의 캐시 메모리를
**분리형 캐시(split cache)** 라고 한다.

<img src="https://github.com/user-attachments/assets/00797109-fce4-4fb9-b2e4-2da5076e3a98" width="520"/><br/>
<br/>

#### [캐시 히트와 캐시 미스]
캐시 메모리는 메모리보다 용량이 작기 때문에 메모리에 있는 모든 내용을 캐시 메모리에 가져와 저장할 수는 없다.
메모리가 보조기억장치의 일부를 복사하여 저장하는 것처럼 캐시 메모리도 메모리의 일부를 복사하여 저장한다. 그렇다면 캐시 메모리에는 무엇을 저장해야 할까?
보조저장장치가 (전원이 꺼져도)'보관할 것'을 저장하고, 메모리가 '실행 중인 것'을 저장한다면 캐시 메모리는 **CPU가 사용할 법한 것**을 저장한다.

이렇게 캐시 메모리가 예측하여 저장한 데이터가 CPU에 의해 실제로 사용되는 것을 **캐시 히트(cache hit)** 라고 하며, 반대로 자주 사용될 것으로 예측하여 캐시 메모리에 저장했지만 틀린 예측으로
인해 CPU가 메모리로부터 필요한 데이터를 직접 가져와야 하는 경우를 **캐시 미스(cache miss)**라고 한다.
만약 캐시 미스가 발생한다면 캐시 메모리의 이점을 활용할 수 없게 되고, 캐시 히트에 비해 CPU의 성능이 떨어지게 된다.

<img src="https://github.com/user-attachments/assets/91648b52-7284-4a01-9b9f-b03cc0090091" width="600"/><br/>
<br/>

참고로, 캐시가 히트되는 비율을 **캐시 적중률(cache hit ratio)** 이라고 하며, 다음과 같이 계산한다. 범용적으로 사용되는 컴퓨터의 캐시 적중률은 대략 85 ~ 95% 이상이다.

```
캐시 히트 횟수 / (캐시 히트 횟수 + 캐시 미스 횟수)
```

#### [참조 지역성의 원리]
캐시 메모리의 이점을 제대로 활용하려면 CPU가 사용할 법한 데이터를 제대로 예측해서 캐시 적중률을 높여야 한다. 그렇다면 CPU가 사용할 법한 데이터는 어떻게 예측할 수 있을까?
캐시 메모리는 **참조 지역성의 원리(locality of reference, principle of locality)** 라는 특정한 원칙에 따라 메모리로부터 가져올 데이터를 결정한다.
참조 지역성의 원리란 CPU가 메모리에 접근할 때 보이는 다음과 같은 주된 경향을 의미한다.

- **시간 지역성** : CPU는 최근에 접근했던 메모리 공간에 다시 접근하려는 경향이 있다.
- **공간 지역성** : CPU는 접근한 메모리 공간의 근처에 접근하려는 경향이 있다.

시간 지역성을 가장 잘 보이는 사례는 프로그래밍 언어의 **변수**이다. 일반적으로 변수에 저장된 값은 한 번만 사용되지 않고, 프로그램이 실행되는 동안 여러 번 사용된다.
즉, CPU는 최근에 접근했던 (변수가 저장된)메모리 공간에 여러 번 다시 접근할 수 있다. 이렇게 최근에 접근했던 메모리 공간에 다시 접근하려는 경향을 **시간 지역성(temporal locality)** 이라고 한다.

한편, 접근한 메모리 공간의 근처에 접근하려는 경향인 **공간 지역성(spatial locality)** 을 단적으로 보여주는 사례는 **배열**이다. 예를 들어보자.
다음 두 코드는 공간 지역성을 고려한 파이썬 코드와 그렇지 못한 파이썬 코드이다. 파이썬 코드의 문법을 모르더라도 주석을 통해 코드의 의미만 파악해보자.

#### [arch/locality_good.py]
```python
matrix = [[0] * 20000 for _ in range(20000)]      # 2만 ✕ 2만 크기의 2차원 배열 생성

for i in range(20000):                            # i를 0부터 19999까지 반복
    for j in range(20000):                        # j를 0부터 19999까지 반복
        matrix[i][j] = 1                          # matrix[i][j]를 1로 갱신
```

#### [arch/locality_bad.py]
```python
matrix = [[0] * 20000 for _ in range(20000)]      # 2만 ✕ 2만 크기의 2차원 배열 생성

for i in range(20000):                            # i를 0부터 19999까지 반복
    for j in range(20000):                        # j를 0부터 19999까지 반복
        matrix[j][i] = 1                          # matrix[j][i]를 1로 갱신
```

두 코드는 언뜻 비슷해 보이지만, 각 코드의 마지막 행만 다르다. 첫 번째 코드는 메모리에 순차적으로 접근하고, 두 번째 코드는 순차적으로 접근하지 않는다.
메모리 내에 일렬로 저장될 때 2차원 배열은 다음 그림과 같이 행과 열의 순으로 저장된다. 이해를 돕기 위해 4✕4 크기의 2차원 배열을 예시로 살펴보자.

<img src="https://github.com/user-attachments/assets/a70edd35-5062-43fd-ad8e-117e14201582" width="400"/><br/>
<br/>

첫 번째 코드는 이중 반복문을 실행할 때 한 행의 모든 열을 순회하고, 그 다음 행의 모든 열을 순회하는 방식으로 2차원 배열에 접근하지만, 두 번째 코드는 한 열의 모든 행을 순회하고,
그 다음 열의 모든 행을 순회하는 방식으로 2차원 배열에 접근한다.

<img src="https://github.com/user-attachments/assets/40fb9dbc-5536-4f90-a327-286ffaaf99ff" width="620"/><br/>
<br/>

첫 번째 코드와 두 번째 코드의 메모리 접근은 공간 지역성 측면에서 차이가 있어, 실제 실행 속도가 크게 차이가 난다. 파이썬에서 코드를 작성해 실행 속도를 직접 확인해 보자.

#### [캐시 메모리의 쓰기 정책과 일관성]
지금까지는 캐시 메모리가 메모리로부터 데이터를 읽어 들이는 상황을 위주로 설명했다면, 이번에는 캐시 메모리에 데이터를 쓰는 경우에 대해 알아보자.
CPU가 캐시 메모리에 데이터를 쓸 때는 캐시 메모리에 새롭게 쓰여진 데이터와 메모리 상의 데이터가 일관성을 유지해야 한다.

예를 들어 현재 메모리 1000번지에 200이라는 값이 저장되어 있고, 이 값이 캐시 메모리에도 저장되어 있다고 가정해 보자.
CPU가 이 값에 접근하고자 할 때는 당연히 캐시 메모리를 통해 값을 얻어낼 것이다. 이때 만약 CPU가 이 값을 200에서 300으로 바꾸고 싶다면 어떻게 해야 할까?

<img src="https://github.com/user-attachments/assets/47d31c00-be9e-4bdd-b887-321813e4c48e" width="370"/><br/>
<br/>
곧장 메모리 1000번지로 달려가 값을 300으로 바꾸는 것은 좋은 생각이 아니다.
현재 CPU는 1000번지 값을 얻기 위해 캐시 메모리를 참조하고 있기 때문에 메모리 1000번지 값을 무작정 300으로 바꾼다면 다음과 같은 명령어를 수행할 때 예상치 못한 결과가 나타날 수 있기 때문이다.

<img src="https://github.com/user-attachments/assets/c2c12282-6911-4daf-ab20-5414a73658ac" width="600"/><br/>
<br/>

이를 방지하기 위한 방법에는 크게 2가지가 있다. 서로 각기 다른 장단점을 갖고 있는데, 하나는 캐시 메모리와 메모리에 동시에 쓰는 방법이 있을 수 있다.
이를 **즉시 쓰기(write-through)** 라고 한다.
즉시 쓰기는 메모리를 항상 최신 상태로 유지하여 캐시 메모리와 메모리 간의 일관성이 깨지는 상황을 방지할 수 있지만, 데이터를 쓸 때마다 메모리를 참조해야 하므로 버스의 사용 시간과 쓰기 시간이 늘어난다는
단점이 있다. 메모리 접근을 최소화하기 위해 캐시 메모리를 만들었는데, 데이터를 쓸 때마다 메모리와 캐시 메모리에 동시에 접근해야 한다면 캐시 메모리를 둔 효율이 떨어질 것이다.

<img src="https://github.com/user-attachments/assets/9eb8fb24-2bed-4eff-b94d-838eb263e0db" width="370"/><br/>
<br/>

또 다른 방법으로는 캐시 메모리에만 값을 써 두었다가 추후 수정된 데이터를 한 번에 메모리에 반영하는 방법이 있다. 이를 **지연 쓰기(write-back)** 라고 한다.
메모리 접근 횟수를 줄일 수 있어 즉시 쓰기 방식에 비해 속도는 더 빠르지만, 메모리와 캐시 메모리 간의 일관성이 깨질 수 있다는 위험을 감수해야 한다.

<img src="https://github.com/user-attachments/assets/ff5eee6a-dc50-4c1a-8977-9ed0c1a412d8" width="370"/><br/>
<br/>

캐시 메모리와 메모리 간의 불일치만 해결해야 하는 것이 아니다. 때로는 다른 코어가 사용하는 캐시 메모리와의 불일치도 발생할 수 있다.
자칫 각기 다른 코어가 서로 다른 데이터를 대상으로 작업할 수 있기 때문이다.

<img src="https://github.com/user-attachments/assets/3fedb5f6-4936-4ac0-9cb1-b438906692de" width="450"/><br/>
<br/>

> 이러한 문제를 해결하기 위해 '캐시 일관성 프로토콜'이라는 개념이 있다.

강조하고 싶은 것은 이것이다. 캐시 메모리를 사용한다는 것, 나아가 캐싱을 한다는 것은 데이터 접근에 있어 어느 정도의 빠른 성능은 보장할 수 있지만, 그와 동시에 데이터의 일관성을 유지하기 위한 책임이
따르는 방식이라는 것이다.

'캐시'라는 용어는 이후(ch5:네트워크)에서도 등장할 예정이다. 이후 학습할 캐시의 개념도 지금의 캐시 메모리와 본질적으로 다르지 않다.
**자주 사용할 법한 대상을 가까이 위치시킴으로써 성능 향상을 꾀한다**는 점에서 같다. 이 경우도 마찬가지이다.
캐싱을 할 때는 언제나 캐시된 데이터와 원본 데이터 간의 불일치와 데이터의 일관성을 고려해야 한다.
네트워크 캐시에서 어떻게 캐시된 데이터와 원본 데이터의 불일치를 다뤄 일관성을 고려하는지에 대해서는 이후에 알아보도록 한다.

<img src="https://github.com/user-attachments/assets/3ff6e66d-873c-465d-9b9f-28789f018aae" width="430"/><br/>